# ETL Data Connector for URLHaus 
This directory contains a custom Extractâ€“Transformâ€“Load (ETL) Python script that retrieves csv data from this <a href="https://urlhaus.abuse.ch/downloads/csv_online/">URLHaus endpoint</a> and stores it both in mongoDB and as a csv file.


## âœ… Submission Checklist

- [âœ…] Choose a data provider (API) and understand its documentation
- [NA] Secure all API credentials using a `.env` file
- [âœ…] Build a complete ETL pipeline: Extract â†’ Transform â†’ Load (into MongoDB)
- [âœ…] Test and validate your pipeline (handle errors, invalid data, rate limits, etc.)
- [âœ…] Follow the provided Git project structure
- [âœ…] Write a clear and descriptive `README.md` in your folder with API details and usage instructions
- [âœ…] **Include your name and roll number in your commit messages**
- [âœ…] Push your code to your branch and submit a Pull Request

---

## ğŸ“¦ Project Structure

/Avaneesh_Koushik_31222225001018_A/
â”œâ”€â”€ etl_connector.py
â”œâ”€â”€ ENV_TEMPLATE
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md


- **`ENV_TEMPLATE`**: Sample .env file
- **`etl_connector.py`**: Main ETL script.
- **`requirements.txt`**: List all Python dependencies.
- **`README.md`**: Instructions for your connector.



## ğŸ—ƒï¸ MongoDB Guidelines

- Used one MongoDB collection per connector (e.g., `connectorname_raw`).
- Stored ingestion timestamps for audit and update purposes.

# Sample Data Stored:
{
  "_id": {
    "$oid": "689ce121e32e541f9bb20304"
  },
  "id": "3602184",
  "dateadded": "2025-08-13 18:48:08",
  "url": "MALWARE_LINK",
  "url_status": "online",
  "last_online": "2025-08-13 18:48:08",
  "threat": "malware_download",
  "tags": "32-bit,elf,mips,Mozi",
  "urlhaus_link": "https://urlhaus.abuse.ch/url/3602184/",
  "reporter\r": "geenensp",
  "ingested_at": {
    "$date": "2025-08-14T00:31:53.570Z"
  }
}
---

## ğŸ§ª Testing & Validation

- Check for invalid responses, empty payloads, rate limits, and connectivity issues.
- Ensure data is correctly inserted into MongoDB.

---



